services:
  nginx:
    container_name: nginx_proxy
    image: nginx
    volumes:
      - ./html:/usr/share/nginx/html
      - ./html/default.conf:/etc/nginx/conf.d/default.conf
    ports:
      - 80:80
    depends_on:
      - java_app
  java_app:
    container_name: java_backend
    build: ./spark-realtime/publisher-realtime
    ports:
      - 8090:8090

  es:
    image: docker.elastic.co/elasticsearch/elasticsearch:8.17.4
    container_name: es
    volumes:
      - ./docker/elastic/es-plugins/ik:/usr/share/elasticsearch/plugins/ik
      - esdata:/usr/share/elasticsearch/data
    ports:
      - 127.0.0.1:9200:9200
    environment:
      - node.name=es
      - cluster.name=elasticsearch
      - discovery.type=single-node
      - bootstrap.memory_lock=true
      - xpack.security.enabled=false
      - xpack.security.http.ssl.enabled=false
      - xpack.security.transport.ssl.enabled=false
      - TZ=Asia/Shanghai
    mem_limit: 2g
    restart: unless-stopped
    ulimits:
      memlock:
        soft: -1
        hard: -1
    healthcheck:
      test: ["CMD-SHELL", "curl -s http://localhost:9200 | grep -q 'You Know, for Search'"]
      interval: 10s
      timeout: 10s
      retries: 10
  kibana:
    depends_on:
      es:
        condition: service_healthy
    image: docker.elastic.co/kibana/kibana:8.17.4
    container_name: kibana
    volumes:
      - kibanadata:/usr/share/kibana/data
    ports:
      - 5601:5601
    environment:
      - SERVERNAME=kibana
      - ELASTICSEARCH_HOSTS=http://es:9200
      - xpack.security.enabled=false
      - xpack.reporting.roles.enabled=false
      - TZ=Asia/Shanghai
    mem_limit: 1g
    restart: unless-stopped
    healthcheck:
      test: [ "CMD-SHELL", "curl -s -I http://localhost:5601 | grep -q 'HTTP/1.1 302 Found'", ]
      interval: 10s
      timeout: 10s
      retries: 120

  broker:
    image: apache/kafka-native:3.9.0
    container_name: broker
    restart: on-failure
    hostname: broker
    ports:
      - 9092:9092
    volumes:
      - kafkadata:/home/appuser
    command: >
      sh -c "chown -R 1000:1000 /home/appuser && /etc/kafka/docker/run"
    environment:
      KAFKA_NODE_ID: 1
      CLUSTER_ID: '4L6g3nShT-eMCtK--X86sw'
      KAFKA_PROCESS_ROLES: 'broker,controller'
      KAFKA_LISTENERS: 'CONTROLLER://:29093,PLAINTEXT_HOST://:9092,PLAINTEXT://:19092'
      KAFKA_ADVERTISED_LISTENERS: 'PLAINTEXT_HOST://localhost:9092,PLAINTEXT://broker:19092'
      KAFKA_INTER_BROKER_LISTENER_NAME: 'PLAINTEXT'
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: 'CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT'
      KAFKA_CONTROLLER_LISTENER_NAMES: 'CONTROLLER'
      KAFKA_CONTROLLER_QUORUM_VOTERS: '1@broker:29093'
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1
      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0
      KAFKA_NUM_PARTITIONS: 4
      KAFKA_LOG_DIRS: '/home/appuser'
      TZ: Asia/Shanghai
  collection:
    build: ./docker/log-collection
    container_name: collection
    depends_on:
      - broker
    environment:
      - TZ=Asia/Shanghai
  valkey:
    image: valkey/valkey:8.0
    container_name: valkey
    restart: on-failure
    ports:
      - 127.0.0.1:6379:6379
    volumes:
      - valkeydata:/data
    environment:
      - TZ=Asia/Shanghai
  maxwell:
    image: zendesk/maxwell:v1.29.2
    container_name: maxwell
    restart: on-failure
    environment:
      - TZ=Asia/Shanghai
    depends_on:
      - broker
      - maxwell_mysql
    command: >
      bin/maxwell --config config.properties
    volumes:
      - ./docker/maxwell/config.properties:/app/config.properties
  maxwell_mysql:
    image: mysql/mysql-server:5.7
    container_name: maxwell_mysql
    hostname: maxwell_mysql
    restart: on-failure
    ports:
      - 3306:3306
    environment:
      MYSQL_ROOT_HOST: '%'
      MYSQL_ROOT_PASSWORD: 123456
      MYSQL_DATABASE: gmall
      TZ: Asia/Shanghai
    volumes:
      - ./docker/maxwell/maxwell.sql:/docker-entrypoint-initdb.d/maxwell.sql
      - ./docker/maxwell/gmall.sql:/docker-entrypoint-initdb.d/gmall.sql
      - ./docker/maxwell/my.cnf:/etc/my.cnf
      - maxwell_mysqldata:/var/lib/mysql

  namenode:
    image: bde2020/hadoop-namenode:2.0.0-hadoop3.2.1-java8
    container_name: namenode
    hostname: namenode
    restart: on-failure
    ports:
      - 9870:9870
      - 9010:9000
    volumes:
      - hadoop_namenode:/hadoop/dfs/name
    environment:
      - CLUSTER_NAME=test
    env_file:
      - ./docker/docker-hadoop-spark/hadoop.env

  datanode:
    image: bde2020/hadoop-datanode:2.0.0-hadoop3.2.1-java8
    container_name: datanode
    hostname: datanode
    restart: on-failure
    volumes:
      - hadoop_datanode:/hadoop/dfs/data
    environment:
      SERVICE_PRECONDITION: "namenode:9870"
    ports:
      - "9864:9864"
    env_file:
      - ./docker/docker-hadoop-spark/hadoop.env

  resourcemanager:
    image: bde2020/hadoop-resourcemanager:2.0.0-hadoop3.2.1-java8
    container_name: resourcemanager
    hostname: resourcemanager
    restart: on-failure
    environment:
      SERVICE_PRECONDITION: "namenode:9000 namenode:9870 datanode:9864"
    env_file:
      - ./docker/docker-hadoop-spark/hadoop.env

  nodemanager:
    image: bde2020/hadoop-nodemanager:2.0.0-hadoop3.2.1-java8
    container_name: nodemanager
    hostname: nodemanager
    restart: on-failure
    environment:
      SERVICE_PRECONDITION: "namenode:9000 namenode:9870 datanode:9864 resourcemanager:8088"
    env_file:
      - ./docker/docker-hadoop-spark/hadoop.env

  historyserver:
    image: bde2020/hadoop-historyserver:2.0.0-hadoop3.2.1-java8
    container_name: historyserver
    hostname: historyserver
    restart: on-failure
    environment:
      SERVICE_PRECONDITION: "namenode:9000 namenode:9870 datanode:9864 resourcemanager:8088"
    volumes:
      - hadoop_historyserver:/hadoop/yarn/timeline
    env_file:
      - ./docker/docker-hadoop-spark/hadoop.env

  spark-master:
    image: bde2020/spark-master:3.0.0-hadoop3.2
    container_name: spark-master
    hostname: spark-master
    depends_on:
      - namenode
      - datanode
    ports:
      - "4040:4040"
      - "8080:8080"
      - "7077:7077"
    environment:
      - INIT_DAEMON_STEP=setup_spark
      - CORE_CONF_fs_defaultFS=hdfs://namenode:9000
      - TZ=Asia/Shanghai
    volumes:
      - spark_workplace:/root

  spark-worker-1:
    image: bde2020/spark-worker:3.0.0-hadoop3.2
    container_name: spark-worker-1
    hostname: spark-worker-1
    depends_on:
      - spark-master
    ports:
      - "8081:8081"
    environment:
      - "SPARK_MASTER=spark://spark-master:7077"
      - CORE_CONF_fs_defaultFS=hdfs://namenode:9000
      - TZ=Asia/Shanghai
    volumes:
      - spark_workplace:/root

  hive-server:
    image: bde2020/hive:2.3.2-postgresql-metastore
    container_name: hive-server
    hostname: hive-server
    depends_on:
      - namenode
      - datanode
      - hive-metastore
    env_file:
      - ./docker/docker-hadoop-spark/hadoop-hive.env
    environment:
      HIVE_CORE_CONF_javax_jdo_option_ConnectionURL: "jdbc:postgresql://hive-metastore/metastore"
      SERVICE_PRECONDITION: "hive-metastore:9083"
    ports:
      - "10000:10000"

  hive-metastore:
    image: bde2020/hive:2.3.2-postgresql-metastore
    container_name: hive-metastore
    hostname: hive-metastore
    depends_on:
      - namenode
      - datanode
      - hive-metastore-postgresql
    env_file:
      - ./docker/docker-hadoop-spark/hadoop-hive.env
    command: /opt/hive/bin/hive --service metastore
    environment:
      SERVICE_PRECONDITION: "namenode:9870 datanode:9864 hive-metastore-postgresql:5432"
    ports:
      - "9083:9083"

  hive-metastore-postgresql:
    image: bde2020/hive-metastore-postgresql:2.3.0
    container_name: hive-metastore-postgresql
    hostname: hive-metastore-postgresql
    environment:
      - TZ=Asia/Shanghai
    ports:
      - "5432:5432"

  presto-coordinator:
    image: shawnzhu/prestodb:0.181
    container_name: presto-coordinator
    hostname: presto-coordinator
    environment:
      - TZ=Asia/Shanghai
    ports:
      - "8089:8089"

volumes:
  hadoop_namenode:
  hadoop_datanode:
  hadoop_historyserver:
  spark_workplace:
  kafkadata:
  esdata:
  kibanadata:
  valkeydata:
  maxwell_mysqldata:

